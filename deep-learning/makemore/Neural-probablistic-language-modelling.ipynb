{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff14c4f-e1ba-4f12-9419-437525f5fa61",
   "metadata": {},
   "source": [
    "# A Neural Probabilistif Language Model\n",
    "\n",
    "## Terms to understand\n",
    "\n",
    "### Generalisation\n",
    "This is the ability of a model to perform well on unseen data.\n",
    "\n",
    "### Discrete Data\n",
    "Data, where a slight change can result in a totally different interpretation/representation\n",
    "\n",
    "For example:\n",
    "If we want to represent the words of a sentence in numbers so models can be trained, A word like\n",
    "\"Cat\" can be assigned 21, and another word \"Kitten\" can be assigned word 82. Even though they are \n",
    "semantically similar, still their numeric representaions are so far, that the model won't generalise in the real world.\n",
    "\n",
    "This gets worse when we try to increase the context of the model, because there are too many possible combinations of the words.\n",
    "\n",
    "### Continuous Data\n",
    "Representing that data in a form of \"dense vectors\", where each word (or combination) is represented as a long vector makes things better. Now, we can provide similar looking vectors to semantically similar words like \"Cat\" and \"Kitten\" so that the model can generalise well in the real world.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e2736-ffc1-48eb-ab08-7d2cb7725944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
